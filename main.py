from __future__ import annotations

from typing import Dict, List, Any, Tuple
import io
from pathlib import Path
from functools import lru_cache

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Body, Response
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# ==== Lazy imports (åŠ é€Ÿ Render å†·å•Ÿå‹•) ====
# åªæœ‰åœ¨çœŸçš„éœ€è¦è·‘æ¨¡å‹/åˆ†ç¾¤æ™‚æ‰è¼‰å…¥ numpy/pandas/sklearnï¼Œ
# è®“é¦–é èˆ‡éœæ…‹é é¢å¹¾ä¹ç§’é–‹ã€‚

@lru_cache(maxsize=1)
def _lazy_import_ml() -> None:
    global np, pd
    global ColumnTransformer, Pipeline, StandardScaler, OneHotEncoder, SimpleImputer
    global RandomForestRegressor, ElasticNetCV, train_test_split, r2_score, mean_absolute_error, KMeans

    import numpy as np  # type: ignore
    import pandas as pd  # type: ignore

    from sklearn.compose import ColumnTransformer  # type: ignore
    from sklearn.pipeline import Pipeline  # type: ignore
    from sklearn.preprocessing import StandardScaler, OneHotEncoder  # type: ignore
    from sklearn.impute import SimpleImputer  # type: ignore
    from sklearn.ensemble import RandomForestRegressor  # type: ignore
    from sklearn.linear_model import ElasticNetCV  # type: ignore
    from sklearn.model_selection import train_test_split  # type: ignore
    from sklearn.metrics import r2_score, mean_absolute_error  # type: ignore
    from sklearn.cluster import KMeans  # type: ignore

    # å°‡å°å…¥çš„æ¨¡çµ„/é¡åˆ¥æ›åˆ°å…¨åŸŸï¼Œè®“åŸæœ¬çš„ helper functions ä¸ç”¨å¤§æ”¹
    globals()["np"] = np
    globals()["pd"] = pd
    globals()["ColumnTransformer"] = ColumnTransformer
    globals()["Pipeline"] = Pipeline
    globals()["StandardScaler"] = StandardScaler
    globals()["OneHotEncoder"] = OneHotEncoder
    globals()["SimpleImputer"] = SimpleImputer
    globals()["RandomForestRegressor"] = RandomForestRegressor
    globals()["ElasticNetCV"] = ElasticNetCV
    globals()["train_test_split"] = train_test_split
    globals()["r2_score"] = r2_score
    globals()["mean_absolute_error"] = mean_absolute_error
    globals()["KMeans"] = KMeans
# ==== å›ºå®šç‰ˆå››ç¾¤ã€Œäººæ‰ç¾…ç›¤ã€æ¨¡å‹å¡ï¼ˆå…ˆçµ¦é è¨­æ–‡å­—ï¼Œä½ ä¹‹å¾Œå¯ä»¥æ”¹ï¼‰ ====

CLUSTER_CARDS: Dict[int, Dict[str, Any]] = {
    0: {
        "key": "growth_high_engagement",
        "name": "é«˜æŠ•å…¥æˆé•·å‹",
        "short_title": "ä¸»å‹•å­¸ç¿’ï¼Œé‚„æœ‰å¾ˆå¤§æˆé•·ç©ºé–“",
        "summary": "é€šå¸¸å­¸ç¿’å‹•æ©Ÿé«˜ã€è¨“ç·´åƒèˆ‡åº¦ä½³ï¼Œç¸¾æ•ˆå·²ä¸éŒ¯ä½†ä»åœ¨ä¸Šå‡è»Œé“ã€‚",
        "dev_focus": [
            "æä¾›å…·æŒ‘æˆ°æ€§çš„å°ˆæ¡ˆæˆ–è¼ªèª¿æ©Ÿæœƒ",
            "è¨­è¨ˆæ˜ç¢ºçš„æ™‰å‡ï¼è·æ¶¯æ™‰ç´šè·¯å¾‘",
            "å®šæœŸå›é¥‹ï¼Œå”åŠ©èª¿æ•´å­¸ç¿’é‡é»"
        ],
        "risk_alert": [
            "é¿å…é•·æœŸé«˜æŠ•å…¥å»å‡é·åœæ»¯ï¼Œå°è‡´å‹•æ©Ÿä¸‹æ»‘",
            "æ³¨æ„å·¥ä½œè² è·æ˜¯å¦éé«˜ï¼Œå½±éŸ¿å·¥ä½œç”Ÿæ´»å¹³è¡¡"
        ],
        "suggestions": [
            "å®‰æ’ä¸€ä½è³‡æ·±å°å¸«ï¼Œå”åŠ©è¦åŠƒæœªä¾† 1â€“2 å¹´çš„æˆé•·ç›®æ¨™",
            "è®“ä»–åƒèˆ‡è·¨éƒ¨é–€å°ˆæ¡ˆï¼Œæ“´å¤§å½±éŸ¿ç¯„åœ",
            "æ­é…æ˜ç¢ºçš„æŠ€èƒ½èªè­‰æˆ–å¾½ç« åˆ¶åº¦ï¼Œå¼·åŒ–æˆå°±æ„Ÿ"
        ]
    },
    1: {
        "key": "steady_veteran",
        "name": "ç©©å¥è³‡æ·±å‹",
        "short_title": "å¹´è³‡æ·±ã€è¡¨ç¾ç©©å®šçš„é—œéµæ”¯æŸ±",
        "summary": "åœ¨çµ„ç¹”å¹´è³‡è¼ƒé•·ã€ç¸¾æ•ˆç©©å®šï¼Œæ˜¯åœ˜éšŠä¸­çš„ç©©å®šåŠ›é‡èˆ‡çŸ¥è­˜ä¾†æºã€‚",
        "dev_focus": [
            "å¼·åŒ–çŸ¥è­˜å‚³æ‰¿èˆ‡æ•™ç·´è§’è‰²",
            "å”åŠ©æ›´æ–°æŠ€èƒ½ï¼Œé¿å…èˆ‡æ–°æŠ€è¡“è„«ç¯€",
            "é¼“å‹µåƒèˆ‡åˆ¶åº¦ï¼æµç¨‹å„ªåŒ–å°ˆæ¡ˆ"
        ],
        "risk_alert": [
            "ç•™æ„æ˜¯å¦å‡ºç¾å‹•èƒ½ä¸‹æ»‘æˆ–å°è®Šé©æŠ—æ‹’",
            "é¿å…åªè¢«è¦–ç‚ºã€ç©©å®šäººåŠ›ã€è€Œç¼ºä¹æˆé•·æ©Ÿæœƒ"
        ],
        "suggestions": [
            "è¨­è¨ˆã€è³‡æ·±å“¡å·¥ mentor è¨ˆç•«ã€ï¼Œç”±ä»–å¸¶æ–°åŒä»",
            "é‚€è«‹åƒèˆ‡å…§éƒ¨è¨“ç·´èª²ç¨‹æˆèª²æˆ–å…±å‚™",
            "åœ¨ç¸¾æ•ˆå°è©±ä¸­åŠ å…¥ã€å‚³æ‰¿èˆ‡å½±éŸ¿åŠ›ã€çš„æŒ‡æ¨™"
        ]
    },
    2: {
        "key": "high_pressure_risky",
        "name": "é«˜å£“é«˜é¢¨éšªå‹",
        "short_title": "è² è·é«˜ã€ç¸¾æ•ˆå¯èƒ½å…©æ¥µï¼Œéœ€è¦é¢¨éšªç®¡ç†",
        "summary": "å¸¸å‡ºç¾é«˜å·¥æ™‚ã€é«˜å£“åŠ›æˆ–é »ç¹åŠ ç­ï¼Œç¸¾æ•ˆæœ‰æ™‚äº®çœ¼ã€æœ‰æ™‚ä¸ç©©å®šã€‚",
        "dev_focus": [
            "èª¿æ•´å·¥ä½œè² è·èˆ‡è§’è‰²å®šä½ï¼Œé¿å…é•·æœŸéå‹",
            "å¼•å…¥å£“åŠ›ç®¡ç†èˆ‡å¿ƒç†è³‡æº",
            "æ˜ç¢ºè¨­å®šå„ªå…ˆé †åºèˆ‡å¯è¢«æ‹’çµ•çš„ç•Œç·š"
        ],
        "risk_alert": [
            "å€¦æ€ é¢¨éšªé«˜ï¼Œå¯èƒ½çªç„¶é›¢è·æˆ–è¡¨ç¾é©Ÿé™",
            "å®¹æ˜“å½±éŸ¿åœ˜éšŠæ°›åœï¼Œè®“å£“åŠ›æ–‡åŒ–æ“´æ•£"
        ],
        "suggestions": [
            "æª¢è¦–æ‰‹ä¸Šçš„ä»»å‹™èˆ‡ KPIï¼Œå”åŠ©åˆªæ¸›éé—œéµå·¥ä½œ",
            "æä¾›å½ˆæ€§å·¥æ™‚æˆ–ä¼‘å‡å®‰æ’ï¼Œè®“ä»–æœ‰æ¢å¾©ç©ºé–“",
            "HR å®šæœŸ 1:1 check-inï¼Œè¿½è¹¤å£“åŠ›èˆ‡å¥åº·ç‹€æ³"
        ]
    },
    3: {
        "key": "emerging_talent",
        "name": "æ–°ç§€æ½›åŠ›å‹",
        "short_title": "å¹´è³‡è¼ƒçŸ­ã€æ½›åŠ›æ˜é¡¯ï¼Œéœ€è¦è¢«å¥½å¥½æ ½åŸ¹",
        "summary": "å‰›åŠ å…¥æˆ–å¹´è³‡å°šçŸ­ï¼Œè¡¨ç¾å·²å±•ç¾æ½›åŠ›ï¼Œä½†ä»åœ¨æ‘¸ç´¢éšæ®µã€‚",
        "dev_focus": [
            "å¿«é€Ÿè£œé½Šæ ¸å¿ƒæŠ€èƒ½èˆ‡åˆ¶åº¦çŸ¥è­˜",
            "å»ºç«‹æ¸…æ¥šçš„æœŸæœ›èˆ‡å›é¥‹é »ç‡",
            "è®“ä»–æœ‰å°è¦æ¨¡è©¦éŒ¯èˆ‡å˜—è©¦çš„ç©ºé–“"
        ],
        "risk_alert": [
            "è‹¥ç¼ºä¹æŒ‡å°èˆ‡å›é¥‹ï¼Œå®¹æ˜“è¿·æƒ˜æˆ–å–ªå¤±ä¿¡å¿ƒ",
            "å¤ªå¿«å£“ä¸Šé—œéµä»»å‹™ï¼Œå¯èƒ½å£“åŠ›éå¤§"
        ],
        "suggestions": [
            "å®‰æ’å…¥è·å¾Œ 3â€“6 å€‹æœˆçš„çµæ§‹åŒ–åŸ¹è¨“èˆ‡ check-point",
            "åœ¨ç¸¾æ•ˆå°è©±ä¸­å¤šé—œæ³¨ã€å­¸ç¿’æ›²ç·šã€è€Œéå–®æ¬¡çµæœ",
            "æ­é…å¸«å¾’åˆ¶æˆ–åŒå„• buddyï¼Œæä¾›æ—¥å¸¸æ”¯æŒ"
        ]
    },
}



try:
    import shap  # type: ignore
    HAS_SHAP = True
except Exception:
    shap = None
    HAS_SHAP = False


app = FastAPI(title="AI Talent Predictor API", version="0.2")

# è®“ Chrome DevTools çš„æ¢æ¸¬è«‹æ±‚å®‰éœä¸€é»ï¼ˆä¸å½±éŸ¿åŠŸèƒ½ï¼‰
@app.get("/.well-known/appspecific/{path:path}")
def _chrome_devtools_noise(path: str):
    return Response(status_code=204)


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health():
    return {"ok": True}

# ===== å…¨åŸŸç‹€æ…‹ï¼šå­˜æ¨¡å‹ã€å‰è™•ç†å™¨ã€æ¬„ä½è³‡è¨Š =====
MODEL_STATE: Dict[str, Any] = {
    "pipe_rf": None,
    "preprocessor": None,
    "numeric_cols": None,
    "categorical_cols": None,
    "feature_cols": None,
    "defaults_all": None,
    "shap_explainer": None,
    "shap_feature_names": None,
    "top_features": None,   # å·²å­˜åœ¨ï¼šçµ¦ Demo é ç”¨
    "metrics": None,        # å·²å­˜åœ¨
    "kmeans": None,         # ğŸ‘ˆ æ–°å¢ï¼šk=4 åˆ†ç¾¤æ¨¡å‹
    "perf_quantiles": None, # ğŸ‘ˆ æ–°å¢ï¼šç”¨ä¾†åˆ¤æ–·é«˜/ä¸­/ä½ç¸¾æ•ˆ band
    # ğŸ”¹ æ–°å¢ï¼šä¿ç•™å®Œæ•´ç‰¹å¾µè³‡æ–™å’Œ yï¼Œçµ¦è‡ªç”±åˆ†ç¾¤ç”¨
    "X_all": None,
    "y_all": None,
    "df_for_cluster": None,   # ğŸ‘ˆ æ–°å¢ï¼šçµ¦è‡ªç”±åˆ†ç¾¤æ²™ç›’ç”¨
}




class PredictRequest(BaseModel):
    features: Dict[str, Any]

class ClusterSandboxRequest(BaseModel):
    k: int


class TalentCompassRequest(BaseModel):
    features: Dict[str, Any]

class ClusterPlayRequest(BaseModel):
    k: int = 4   # é è¨­ 4 ç¾¤



# ===== å…±ç”¨å°å·¥å…· =====
def read_csv_upload(file: UploadFile) -> pd.DataFrame:
    content = file.file.read()
    try:
        df = pd.read_csv(io.BytesIO(content))
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"ç„¡æ³•è®€å– CSVï¼š{e}")
    if df.empty:
        raise HTTPException(status_code=400, detail="CSV å…§å®¹ç‚ºç©º")
    return df


def is_numeric_series(s: pd.Series) -> bool:
    return pd.api.types.is_numeric_dtype(s)


def build_preprocessor(df_features: pd.DataFrame) -> Tuple[ColumnTransformer, List[str], List[str]]:
    num_cols = [c for c in df_features.columns if is_numeric_series(df_features[c])]
    cat_cols = [c for c in df_features.columns if c not in num_cols]

    num_tf = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )
    cat_tf = Pipeline(
        [
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    pre = ColumnTransformer(
        [
            ("num", num_tf, num_cols),
            ("cat", cat_tf, cat_cols),
        ]
    )
    return pre, num_cols, cat_cols


# ===== æ¨¡å‹è¨“ç·´ä¸»æµç¨‹ï¼ˆç…§ä½  Step2 æ”¹æˆå‡½å¼ç‰ˆï¼‰ =====
def train_model(df: pd.DataFrame, test_size: float = 0.2) -> Dict[str, Any]:
    # Render å…è²»/å°æ©Ÿå™¨å¾ˆå®¹æ˜“å› ç‚ºè³‡æ–™å¤ªå¤§æˆ–æ¨¡å‹å¤ªé‡è¢«æ‰“çˆ†ï¼ˆtimeout / OOM â†’ 502ï¼‰ã€‚
    # é€™è£¡å…ˆåšã€Œä¿å®ˆé™é€Ÿã€ï¼šè³‡æ–™æŠ½æ¨£ + è¼•é‡æ¨¡å‹åƒæ•¸ï¼Œè®“è¨“ç·´èƒ½åœ¨çŸ­æ™‚é–“å…§å®Œæˆã€‚
    import time
    t0 = time.time()

    MAX_ROWS = 6000
    n_rows_raw = int(len(df))
    if n_rows_raw > MAX_ROWS:
        # æŠ½æ¨£å¾ŒæŠŠ df è¦†è“‹æ‰ï¼Œé¿å…æ•´ä»½å¤§è¡¨ä¸€ç›´ç•™åœ¨è¨˜æ†¶é«”è£¡
        df = df.sample(n=MAX_ROWS, random_state=42).reset_index(drop=True)
    n_rows_used = int(len(df))

    target_col = "Performance_Score"
    if target_col not in df.columns:
        raise HTTPException(status_code=400, detail="è³‡æ–™ä¸­æ‰¾ä¸åˆ° Performance_Score æ¬„ä½")

    # 1) é¿å…æ´©æ¼ï¼šä¸Ÿæ‰ ID / æ—¥æœŸ / ç›®æ¨™ / é›¢è·ã€æ™‰å‡ã€cluster ç­‰æ¬„ä½
    drop_cols: List[str] = []
    for col in df.columns:
        lower = col.lower()
        if "id" in lower or "date" in lower:
            drop_cols.append(col)

    drop_cols.extend(
        [
            target_col,
            "Resigned",
            "Promotions",
            "Promotion_Last_3_Years",
            "cluster_kmeans_v2",
            "cluster_kmeans",
            "y_true",
            "y_pred",
        ]
    )
    drop_cols = sorted(set([c for c in drop_cols if c in df.columns]))

    y = df[target_col].astype(float)
    feature_cols = [c for c in df.columns if c not in drop_cols]
    if not feature_cols:
        raise HTTPException(status_code=400, detail="ç§»é™¤ ID / ç›®æ¨™æ¬„ä½å¾Œæ²’æœ‰å‰©ä¸‹ä»»ä½•ç‰¹å¾µå¯ç”¨")

    X = df[feature_cols].copy()

    # çµ¦è‡ªç”±åˆ†ç¾¤æ²™ç›’ç”¨ï¼šä¿ç•™ç‰¹å¾µ + ç›®æ¨™
    MODEL_STATE["df_for_cluster"] = df[feature_cols + [target_col]].copy()

    # ğŸ”¹ æ–°å¢ï¼šæŠŠå®Œæ•´ X / y å­˜èµ·ä¾†ï¼Œè®“å¾Œé¢è‡ªç”±åˆ†ç¾¤å¯ä»¥ç”¨åŒä¸€ä»½è³‡æ–™
    MODEL_STATE["X_all"] = X.copy()
    MODEL_STATE["y_all"] = y.copy()

    print(
        f"[train] start rows={n_rows_raw} used={n_rows_used} features={len(feature_cols)} test_size={test_size}",
        flush=True,
    )


    # 2) å‰è™•ç† + åˆ‡è¨“ç·´ / æ¸¬è©¦
    pre, num_cols, cat_cols = build_preprocessor(X)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )

    # 3) æ¨¡å‹ï¼šRender è¼•é‡ç‰ˆåªç•™ RandomForestï¼ˆé¿å… CV é¡æ¨¡å‹åœ¨ Render ä¸Šè·‘å¤ªä¹…/åƒå¤ªå¤šè¨˜æ†¶é«”ï¼‰
    rf = RandomForestRegressor(
        n_estimators=120,
        max_depth=16,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=1,
    )

    pipe_rf = Pipeline([("pre", pre), ("rf", rf)])
    pipe_rf.fit(X_train, y_train)

    # 4) è©•ä¼°æŒ‡æ¨™
    pred_rf_train = pipe_rf.predict(X_train)
    pred_rf_test = pipe_rf.predict(X_test)
    metrics = {
        "rf": {
            "R2_train": float(r2_score(y_train, pred_rf_train)),
            "R2_test": float(r2_score(y_test, pred_rf_test)),
            "MAE_test": float(mean_absolute_error(y_test, pred_rf_test)),
        }
    }


    # === æ–°å¢ Aï¼šç”¨æ•´é«”è³‡æ–™ + å‰è™•ç†å™¨ è¨“ç·´ k=4 åˆ†ç¾¤ ===
    fitted_pre = pipe_rf.named_steps["pre"]
    X_all_trans = fitted_pre.transform(X)   # X æ˜¯å…¨è³‡æ–™ï¼Œä¸åªè¨“ç·´é›†
    kmeans_4 = KMeans(n_clusters=4, random_state=42, n_init=10)
    kmeans_4.fit(X_all_trans)

    # === æ–°å¢ Bï¼šè¨ˆç®—ç¸¾æ•ˆåˆ†å±¤ç”¨çš„åˆ†ä½æ•¸ï¼ˆä½/ä¸­/é«˜ï¼‰ ===
    q_low = float(np.quantile(y_train, 0.33))
    q_high = float(np.quantile(y_train, 0.66))
    perf_quantiles = {"low": q_low, "high": q_high}


    # 5) ç‚ºæ¯å€‹æ¬„ä½ç®—ã€Œé è¨­å€¼ã€ï¼ˆä¸­ä½æ•¸ / çœ¾æ•¸ï¼‰ï¼Œçµ¦é æ¸¬é ç”¨
    defaults_all: Dict[str, Any] = {}
    for col in feature_cols:
        s = X[col]
        if is_numeric_series(s):
            defaults_all[col] = float(s.median())
        else:
            mode = s.mode()
            defaults_all[col] = mode.iloc[0] if not mode.empty else ""

    # 6) SHAP å…¨åŸŸé‡è¦åº¦ï¼ˆèšåˆåˆ°ã€ŒåŸå§‹æ¬„ä½ã€å±¤ç´šï¼‰
    shap_explainer = None
    shap_feature_names: List[str] = []
    shap_global_agg: List[Dict[str, Any]] = []

    # ä¸ç®¡æœ‰æ²’æœ‰ SHAPï¼Œéƒ½å…ˆæŠŠã€Œè½‰æ›å¾Œç‰¹å¾µã€åç¨±å–å‡ºï¼Œæ–¹ä¾¿åšèšåˆ

    fitted_pre = pipe_rf.named_steps["pre"]

    fitted_rf = pipe_rf.named_steps["rf"]


    try:

        feature_names = list(fitted_pre.get_feature_names_out())

    except Exception:

        # é€€ä¸€æ­¥ï¼šè‹¥ sklearn ç‰ˆæœ¬ä¸æ”¯æ´ï¼Œå°±åªèƒ½ä»¥ç´¢å¼•ä»£ç¨±

        fi_len = getattr(fitted_rf, "feature_importances_", np.array([])).shape[0]

        feature_names = [f"f{i}" for i in range(fi_len)]


    def _base_feature_name(trans_name: str) -> str:

        # ColumnTransformer çš„è¼¸å‡ºå¸¸è¦‹æ ¼å¼ï¼šnum__Age / cat__Department_Sales

        if trans_name.startswith("num__"):

            return trans_name.split("__", 1)[1]

        if trans_name.startswith("cat__"):

            sub = trans_name.split("__", 1)[1]

            # OneHotEncoderï¼š<col>_<category>

            # ç”¨ã€Œæœ€é•·å‰ç¶´åŒ¹é…ã€é¿å…æ¬„ä½åæœ¬èº«æœ‰åº•ç·šè¢«åˆ‡å£

            best = None

            for c in cat_cols:

                prefix = f"{c}_"

                if sub == c or sub.startswith(prefix):

                    if best is None or len(c) > len(best):

                        best = c

            return best or sub

        return trans_name


    if HAS_SHAP:

        # âœ… å®Œæ•´ç‰ˆï¼šç”¨ SHAP ç®—å…¨åŸŸé‡è¦åº¦ï¼ˆè¼ƒæº–ï¼Œä½†å¥—ä»¶å¾ˆå¤§ï¼‰

        sample_size = min(800, len(X_train))

        rng = np.random.RandomState(42)

        sample_idx = rng.choice(X_train.index, size=sample_size, replace=False)

        X_train_sample = X_train.loc[sample_idx]

        X_train_trans = fitted_pre.transform(X_train_sample)


        shap_explainer = shap.TreeExplainer(fitted_rf)

        shap_values = shap_explainer.shap_values(X_train_trans)


        mean_abs_shap = np.abs(shap_values).mean(axis=0)

        agg: Dict[str, float] = {}

        for fname, val in zip(feature_names, mean_abs_shap):

            base = _base_feature_name(str(fname))

            agg[base] = agg.get(base, 0.0) + float(val)


        shap_global_agg = [

            {"feature": base, "mean_abs_shap": float(val)}

            for base, val in sorted(agg.items(), key=lambda kv: kv[1], reverse=True)

        ][:15]


        shap_feature_names = feature_names


    else:

        # âœ… è¼•é‡ç‰ˆï¼šä¸ç”¨ SHAPï¼Œæ”¹ç”¨ RandomForest çš„ feature_importances_

        importances = getattr(fitted_rf, "feature_importances_", None)

        if importances is None:

            importances = np.zeros(len(feature_names), dtype=float)


        agg: Dict[str, float] = {}

        for fname, val in zip(feature_names, importances):

            base = _base_feature_name(str(fname))

            agg[base] = agg.get(base, 0.0) + float(val)


        shap_global_agg = [

            {"feature": base, "mean_abs_shap": float(val)}

            for base, val in sorted(agg.items(), key=lambda kv: kv[1], reverse=True)

        ][:15]

    # 7) çµ„åˆå‰ç«¯æœƒç”¨åˆ°çš„æ¬„ä½è³‡è¨Šï¼ˆå«æç¤ºï¼‰
    top_features_with_defaults: List[Dict[str, Any]] = []

    for row in shap_global_agg:
        fname = row["feature"]
        row_out: Dict[str, Any] = dict(row)
        default_val = defaults_all.get(fname)
        row_out["default"] = default_val

        if fname in num_cols:
            row_out["value_type"] = "number"
            # çµ¦ä¸€å€‹å¤§æ¦‚ç¯„åœçš„æç¤º
            hint = None
            try:
                s = df[fname]
                p5 = float(np.nanpercentile(s, 5))
                p95 = float(np.nanpercentile(s, 95))
                med = float(np.nanmedian(s))
                hint = f"å»ºè­°è¼¸å…¥æ•¸å­—ï¼Œå¸¸è¦‹ç¯„åœç´„ {p5:.1f}â€“{p95:.1f}ï¼ˆä¸­ä½æ•¸ {med:.1f}ï¼‰"
            except Exception:
                hint = "è«‹è¼¸å…¥æ•¸å­—ï¼ˆå–®ä½åŒåŸå§‹è³‡æ–™ï¼‰ã€‚"
            row_out["hint"] = hint
        else:
            row_out["value_type"] = "text"
            hint = None
            try:
                if fname in df.columns:
                    s = df[fname].astype(str)
                    top_vals = s.value_counts().head(4).index.tolist()
                    if top_vals:
                        joined = " / ".join(map(str, top_vals))
                        hint = f"å»ºè­°è¼¸å…¥è³‡æ–™ä¸­å‡ºç¾éçš„æ–‡å­—ï¼Œä¾‹å¦‚ï¼š{joined}"
            except Exception:
                pass
            if not hint:
                hint = "è«‹è¼¸å…¥æ–‡å­—ï¼ˆç›¡é‡ä½¿ç”¨è³‡æ–™é›†ä¸­å‡ºç¾éçš„é¡åˆ¥ï¼‰ã€‚"
            row_out["hint"] = hint

        top_features_with_defaults.append(row_out)

    # 8) æŠŠæ¨¡å‹ç›¸é—œç‰©ä»¶å­˜åˆ°å…¨åŸŸç‹€æ…‹ï¼Œæ–¹ä¾¿ /api/predict èˆ‡ /api/model_summary ä½¿ç”¨
    MODEL_STATE["pipe_rf"] = pipe_rf
    MODEL_STATE["preprocessor"] = pipe_rf.named_steps["pre"]
    MODEL_STATE["numeric_cols"] = num_cols
    MODEL_STATE["categorical_cols"] = cat_cols
    MODEL_STATE["feature_cols"] = feature_cols
    MODEL_STATE["defaults_all"] = defaults_all
    MODEL_STATE["shap_explainer"] = shap_explainer
    MODEL_STATE["shap_feature_names"] = shap_feature_names
    MODEL_STATE["top_features"] = top_features_with_defaults  # âœ… ç¾åœ¨è®Šæ•¸å·²ç¶“å…ˆç®—å¥½
    MODEL_STATE["metrics"] = metrics

    # ğŸ‘‡ æ–°å¢ï¼šå­˜ k=4 èˆ‡åˆ†ä½æ•¸
    MODEL_STATE["kmeans"] = kmeans_4
    MODEL_STATE["perf_quantiles"] = perf_quantiles

    secs = time.time() - t0
    print(f"[train] done used={n_rows_used} in {secs:.1f}s", flush=True)

    return {
        "target_col": target_col,
        "n_rows_raw": n_rows_raw,
        "n_rows_used": n_rows_used,
        "n_features": int(len(feature_cols)),
        "metrics": metrics,
        "top_features": top_features_with_defaults,
        "has_shap": HAS_SHAP,
    }



# ===== APIï¼šä¸Šå‚³ CSV â†’ è¨“ç·´æ¨¡å‹ =====
@app.post("/api/train_model")
async def api_train_model(
    file: UploadFile = File(...),
    test_size: float = Form(0.2),
):
    _lazy_import_ml()  # lazy load numpy/pandas/sklearn
    try:
        df = read_csv_upload(file)
        result = train_model(df, test_size=test_size)
        return result
    except HTTPException:
        raise
    except Exception as e:
        import traceback

        print("[train] ERROR:", repr(e), flush=True)
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è¨“ç·´å¤±æ•—ï¼š{e}")


# ===== APIï¼šå–®ä¸€æ¨£æœ¬é æ¸¬ =====
@app.post("/api/predict")
async def api_predict(req: PredictRequest = Body(...)):
    _lazy_import_ml()  # lazy load numpy/pandas/sklearn
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨é æ¸¬é é¢ä¸Šå‚³è³‡æ–™ä¸¦åŸ·è¡Œã€Œå»ºç«‹æ¨¡å‹ã€",
        )

    pipe_rf: Pipeline = MODEL_STATE["pipe_rf"]
    pre = MODEL_STATE["preprocessor"]
    feature_cols: List[str] = MODEL_STATE["feature_cols"]
    defaults_all: Dict[str, Any] = MODEL_STATE["defaults_all"] or {}
    shap_explainer = MODEL_STATE["shap_explainer"]
    shap_feature_names: List[str] = MODEL_STATE["shap_feature_names"] or []

    # ä»¥è¨“ç·´è³‡æ–™çš„ã€Œé è¨­å€¼ã€ç•¶æˆ base å€‹æ¡ˆï¼Œå†ç”¨ä½¿ç”¨è€…è¼¸å…¥è¦†è“‹
    row_data: Dict[str, Any] = {}
    for col in feature_cols:
        row_data[col] = defaults_all.get(col)

    for k, v in req.features.items():
        if k in row_data:
            row_data[k] = v

    X_new = pd.DataFrame([row_data], columns=feature_cols)

    y_pred = float(pipe_rf.predict(X_new)[0])

    # å½±éŸ¿å› å­ï¼ˆTop10ï¼‰
    # - å¦‚æœæœ‰ SHAPï¼šå›å‚³ã€Œå€‹æ¡ˆã€çš„ SHAP Top 10ï¼ˆè¼ƒç²¾æº–ï¼Œä½†è¼ƒåƒè³‡æºï¼‰
    # - å¦‚æœæ²’æœ‰ SHAPï¼šå›å‚³ã€Œå…¨åŸŸã€çš„é‡è¦åº¦ Top 10ï¼ˆè¼•é‡ç‰ˆ fallbackï¼‰
    explain_details: List[Dict[str, Any]] = []

    if HAS_SHAP and shap_explainer is not None and shap_feature_names:
        X_new_trans = pre.transform(X_new)
        shap_values = shap_explainer.shap_values(X_new_trans)[0]
        abs_vals = np.abs(shap_values)
        order = np.argsort(-abs_vals)
        top_k = min(10, len(order))
        for idx in order[:top_k]:
            explain_details.append(
                {
                    "feature": shap_feature_names[idx],
                    "shap_value": float(shap_values[idx]),
                    "abs_shap": float(abs_vals[idx]),
                }
            )
    else:
        # fallbackï¼šç”¨è¨“ç·´éšæ®µç®—å¥½çš„ã€Œå…¨åŸŸé‡è¦åº¦ã€
        top_features = MODEL_STATE.get("top_features") or []
        for rank, row in enumerate(top_features[:10], start=1):
            imp = float(row.get("mean_abs_shap") or 0.0)
            explain_details.append(
                {
                    "feature": row.get("feature", f"feature_{rank}"),
                    "shap_value": imp,   # å‰ç«¯æ²¿ç”¨åŒä¸€æ¬„ä½é¡¯ç¤º
                    "abs_shap": imp,
                }
            )

    return {
        "prediction": y_pred,
        "shap_top_contrib": explain_details,
    }


from fastapi import Body

# ï¼ˆå‰é¢ CLUSTER_CARDSã€BAND_THRESHOLDS ç­‰ä¿æŒä¸å‹•ï¼‰

@app.post("/api/talent_compass_predict")
async def api_talent_compass_predict(req: PredictRequest = Body(...)):
    _lazy_import_ml()  # lazy load numpy/pandas/sklearn
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨é æ¸¬é é¢ä¸Šå‚³è³‡æ–™ä¸¦åŸ·è¡Œã€Œå»ºç«‹æ¨¡å‹ã€",
        )

    pipe_rf: Pipeline = MODEL_STATE["pipe_rf"]
    pre = MODEL_STATE["preprocessor"]
    feature_cols: List[str] = MODEL_STATE["feature_cols"] or []
    defaults_all: Dict[str, Any] = MODEL_STATE["defaults_all"] or {}

    # 1) çµ„ä¸€åˆ—è³‡æ–™ï¼ˆç”¨ defaults ç•¶åº•ï¼Œå†ç”¨ä½¿ç”¨è€…è¼¸å…¥è¦†è“‹ï¼‰
    row_data = {col: defaults_all.get(col) for col in feature_cols}
    for k, v in req.features.items():
        if k in row_data:
            row_data[k] = v

    X_new = pd.DataFrame([row_data], columns=feature_cols)

    # 2) å…ˆç”¨ RF é æ¸¬ç¸¾æ•ˆåˆ†æ•¸
    score = float(pipe_rf.predict(X_new)[0])

    # 3) ç”¨è¨“ç·´æ™‚ç®—å¥½çš„åˆ†ä½æ•¸ï¼Œæ±ºå®š high / medium / low
    perf_q = MODEL_STATE.get("perf_quantiles") or {}
    q_low = perf_q.get("low")
    q_high = perf_q.get("high")

    if q_low is not None and q_high is not None:
        if score >= q_high:
            band_key = "high"
        elif score >= q_low:
            band_key = "medium"
        else:
            band_key = "low"
    else:
        # å¦‚æœæ²’ç®—åˆ°åˆ†ä½æ•¸ï¼Œå°±ç”¨å›ºå®šé–€æª»ç•¶å‚™æ´
        if score >= 4.2:
            band_key = "high"
        elif score >= 3.4:
            band_key = "medium"
        else:
            band_key = "low"

    BAND_LABEL_ZH = {
        "high": "é«˜ç¸¾æ•ˆå¸¶",
        "medium": "ä¸­ç­‰ç¸¾æ•ˆå¸¶",
        "low": "éœ€é—œæ³¨ç¸¾æ•ˆå¸¶",
    }

    # 4) ç”¨ k-means åˆ†ç¾¤ï¼ˆçœŸæ­£å°æ‡‰åˆ°ä½ ç ”ç©¶çš„ 4 ç¾¤ï¼‰
    kmeans: KMeans | None = MODEL_STATE.get("kmeans")
    if kmeans is not None:
        X_new_trans = pre.transform(X_new)
        cluster_id = int(kmeans.predict(X_new_trans)[0])  # 0~3
    else:
        cluster_id = 0  # å‚™æ´

    # 5) ç”¨æ•´æ•¸ cluster_id å»æ‹¿å°æ‡‰çš„äººæ‰å¡
    card = CLUSTER_CARDS.get(cluster_id, {})

    dev_focus = card.get("dev_focus", [])
    # æ³¨æ„ï¼šåŸæœ¬ key å« risk_alert / suggestionsï¼Œé€™é‚Šå¹«ä½ è½‰ä¸€ä¸‹
    risk_alerts = card.get("risk_alert", [])
    hr_tips = card.get("suggestions", [])

    payload = {
        "performance_score": score,
        "performance_band": band_key,
        "performance_level": BAND_LABEL_ZH.get(band_key, "å°šæœªæ¨™å®šç­‰ç´š"),

        "cluster_id": cluster_id,
        "cluster_name": card.get("name", f"ç¬¬ {cluster_id + 1} ç¾¤"),
        "cluster_short_title": card.get("short_title", ""),
        "cluster_summary": card.get("summary", ""),

        "dev_focus": dev_focus,
        "risk_alerts": risk_alerts,
        "hr_tips": hr_tips,

        "cluster_card": card,
    }
    return payload



@app.post("/api/cluster_sandbox")
async def api_cluster_sandbox(req: ClusterSandboxRequest = Body(...)):
    _lazy_import_ml()  # lazy load numpy/pandas/sklearn
    """
    è‡ªç”±åˆ†ç¾¤éŠæ¨‚å ´ç”¨ï¼š
    - ä½¿ç”¨ Demo å·²è¨“ç·´å¥½çš„è³‡æ–™èˆ‡å‰è™•ç†
    - åªèª¿æ•´ kï¼ˆç¾¤æ•¸ï¼‰ï¼Œå›å‚³å„ç¾¤äººæ•¸ã€å¹³å‡ç¸¾æ•ˆèˆ‡ã€Œç›¸å°æ•´é«”çš„ç‰¹å¾µè¼ªå»“ã€
    """
    if MODEL_STATE["df_for_cluster"] is None or MODEL_STATE["preprocessor"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹æˆ–å°šæœªè¼‰å…¥åŸå§‹è³‡æ–™ï¼Œè«‹å…ˆåœ¨ Demo é ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    k = max(2, int(req.k))
    df_cluster: pd.DataFrame = MODEL_STATE["df_for_cluster"].copy()
    feature_cols: List[str] = MODEL_STATE["feature_cols"] or []
    pre = MODEL_STATE["preprocessor"]

    if not feature_cols:
        raise HTTPException(status_code=400, detail="æ‰¾ä¸åˆ°å¯ç”¨ç‰¹å¾µæ¬„ä½ã€‚")

    X_all = df_cluster[feature_cols]
    y_all = df_cluster["Performance_Score"].astype(float)
    n_samples = len(df_cluster)

    # å…ˆç”¨æ—¢æœ‰å‰è™•ç†è½‰æ›ï¼Œå†åš k-means åˆ†ç¾¤
    X_all_trans = pre.transform(X_all)

    km = KMeans(
        n_clusters=k,
        random_state=42,
        n_init="auto",
    )
    labels = km.fit_predict(X_all_trans)
    df_cluster["cluster"] = labels

    overall_mean = float(y_all.mean())

    # ğŸ”¹ åªé‡å°ã€Œæ•¸å€¼ç‰¹å¾µã€åšè¼ªå»“æ¯”è¼ƒ
    numeric_cols: List[str] = MODEL_STATE["numeric_cols"] or []
    numeric_cols = [c for c in numeric_cols if c in df_cluster.columns]

    if numeric_cols:
        global_feature_means = (
            df_cluster[numeric_cols].mean(numeric_only=True).to_dict()
        )
    else:
        global_feature_means = {}

    clusters_out: List[Dict[str, Any]] = []

    # ä¾å„ç¾¤å¹³å‡ç¸¾æ•ˆæ’åºå¾Œå†ç”¢å‡ºèªªæ˜
    cluster_stats: List[Tuple[int, float]] = []
    for cid in range(k):
        mask = df_cluster["cluster"] == cid
        if mask.sum() == 0:
            continue
        mean_perf = float(df_cluster.loc[mask, "Performance_Score"].mean())
        cluster_stats.append((cid, mean_perf))

    # ä¾å¹³å‡ç¸¾æ•ˆç”±é«˜åˆ°ä½æ’åº
    cluster_stats.sort(key=lambda x: x[1], reverse=True)

    for rank_idx, (cid, mean_perf) in enumerate(cluster_stats):
        mask = df_cluster["cluster"] == cid
        sub = df_cluster.loc[mask]
        n_c = int(mask.sum())
        prop = n_c / n_samples if n_samples > 0 else 0.0
        std_perf = float(sub["Performance_Score"].std(ddof=0) or 0.0)

        # å¹³å‡åˆ†æ•¸åœ¨æ•´é«”çš„ç²—ç•¥ç™¾åˆ†ä½
        mean_percentile = float((y_all < mean_perf).mean()) if n_samples > 0 else 0.0

        if rank_idx == 0:
            rank_label = "æ•´é«”é«˜ç¸¾æ•ˆè¼ªå»“"
            comment = "é€™ä¸€ç¾¤çš„å¹³å‡ç¸¾æ•ˆæœ€é«˜ï¼Œé©åˆæ·±å…¥è§€å¯Ÿå…¶å…±åŒç‰¹å¾µï¼Œä½œç‚ºé—œéµäººæ‰è¼ªå»“çš„åƒè€ƒã€‚"
        elif rank_idx == len(cluster_stats) - 1:
            rank_label = "ç›¸å°ä½ç¸¾æ•ˆè¼ªå»“"
            comment = "å¹³å‡ç¸¾æ•ˆæ˜é¡¯ä½æ–¼å…¶ä»–ç¾¤ï¼Œé©åˆæ­é…è¨“ç·´èˆ‡å·¥ä½œè¨­è¨ˆï¼Œæ€è€ƒå¦‚ä½•æ‹‰æŠ¬è¡¨ç¾ã€‚"
        else:
            rank_label = "ä¸­é–“ç¸¾æ•ˆè¼ªå»“"
            comment = "å¹³å‡ç¸¾æ•ˆä»‹æ–¼å…©ç«¯ä¹‹é–“ï¼Œå¯å†ä¾å·¥ä½œå…§å®¹ã€å¹´è³‡ç­‰è®Šé …æ‹†è§£å­è¼ªå»“ã€‚"

        # ğŸ”¹ æœ¬ç¾¤å„æ•¸å€¼ç‰¹å¾µå¹³å‡ï¼Œèˆ‡æ•´é«”æ¯”è¼ƒ
        if numeric_cols:
            feature_means = (
                sub[numeric_cols].mean(numeric_only=True).to_dict()
            )
        else:
            feature_means = {}

        diff_list: List[Dict[str, Any]] = []
        for col in numeric_cols:
            g = global_feature_means.get(col)
            c_mean = feature_means.get(col)
            if g is None or c_mean is None or pd.isna(g) or pd.isna(c_mean):
                continue
            diff_val = float(c_mean - g)
            diff_list.append(
                {
                    "feature": col,
                    "cluster_mean": float(c_mean),
                    "global_mean": float(g),
                    "diff": diff_val,
                }
            )

        diff_list.sort(key=lambda d: abs(d["diff"]), reverse=True)
        top_feature_diff = diff_list[:5]

        clusters_out.append(
            {
                "cluster_id": cid,
                "display_name": f"ç¬¬ {cid + 1} ç¾¤",
                "n_samples": n_c,
                "proportion": float(prop),
                "mean_performance": mean_perf,
                "std_performance": std_perf,
                "mean_percentile": mean_percentile,
                "rank_label": rank_label,
                "comment": comment,
                "feature_means": feature_means,      # â­ æ–°å¢ï¼šæ¯ç¾¤å…¨éƒ¨æ•¸å€¼ç‰¹å¾µå¹³å‡
                "top_feature_diff": top_feature_diff,  # â­ çµ¦å‰ç«¯ç”¨
            }
        )

    return {
        "k": k,
        "n_samples": n_samples,
        "overall_mean_performance": overall_mean,
        "feature_means_global": global_feature_means,  # â­ å…¨é«” baseline
        "clusters": clusters_out,
    }



    
@app.post("/api/cluster_playground")
async def api_cluster_playground(req: ClusterPlayRequest = Body(...)):
    _lazy_import_ml()  # lazy load numpy/pandas/sklearn
    """
    è‡ªç”±ç‰ˆåˆ†ç¾¤éŠæ¨‚å ´ï¼š
    - ä½¿ç”¨ç›®å‰è¨“ç·´å¥½çš„è³‡æ–™ï¼ˆX_all / y_allï¼‰
    - åªç”¨æ•¸å€¼æ¬„ä½åš KMeans åˆ†ç¾¤
    - å›å‚³æ¯ä¸€ç¾¤çš„äººæ•¸ã€æ¯”ä¾‹ã€å¹³å‡ Performance_Scoreã€ä»¥åŠå·®ç•°æœ€å¤§çš„ä¸€äº›æ•¸å€¼ç‰¹å¾µ
    """
    if MODEL_STATE["X_all"] is None or MODEL_STATE["y_all"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåœ¨ç ”ç©¶ç‰ˆé é¢ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    X_all: pd.DataFrame = MODEL_STATE["X_all"]
    y_all: pd.Series = MODEL_STATE["y_all"]

    k = int(req.k)
    if k < 2 or k > 8:
        raise HTTPException(
            status_code=400,
            detail="ç¾¤æ•¸ k å»ºè­°ä»‹æ–¼ 2ï½8ï¼ˆå¤ªå¤šç¾¤å¯è®€æ€§æœƒè®Šå·®ï¼‰ã€‚",
        )

    num_cols: List[str] = MODEL_STATE["numeric_cols"] or []
    if not num_cols:
        raise HTTPException(status_code=400, detail="æ‰¾ä¸åˆ°å¯ç”¨çš„æ•¸å€¼æ¬„ä½ï¼Œç„¡æ³•åˆ†ç¾¤ã€‚")

    # åªç”¨æ•¸å€¼æ¬„ä½åšåˆ†ç¾¤ï¼Œé¿å…æ–‡å­—æ¬„ä½çš„è™•ç†å•é¡Œ
    X_num = X_all[num_cols].copy()

    # ç°¡å–®ç‰ˆå‰è™•ç†ï¼šç¼ºå€¼è£œä¸­ä½æ•¸ï¼‹æ¨™æº–åŒ– â†’ KMeans
    cluster_pipe = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
            # ç”¨æ•´æ•¸ n_initï¼Œé¿å…æŸäº› sklearn ç‰ˆæœ¬ä¸æ”¯æ´ "auto"
            ("kmeans", KMeans(n_clusters=k, random_state=42, n_init=10)),
        ]
    )
    cluster_pipe.fit(X_num)

    labels = cluster_pipe.named_steps["kmeans"].labels_
    n_total = len(X_num)
    overall_means = X_num.mean(axis=0)
    perf_overall = float(y_all.mean())

    clusters_out: List[Dict[str, Any]] = []
    for cid in range(k):
        mask = labels == cid
        n_cluster = int(mask.sum())
        if n_cluster == 0:
            continue

        frac = float(n_cluster / n_total)
        cluster_means = X_num[mask].mean(axis=0)
        perf_mean = float(y_all[mask].mean())

        # æ‰¾å‡ºã€Œæ­¤ç¾¤ vs å…¨é«”ã€å·®ç•°æœ€å¤§çš„å‰ 5 å€‹ç‰¹å¾µ
        diff = (cluster_means - overall_means).abs().sort_values(ascending=False)
        top_features: List[Dict[str, Any]] = []
        for fname in diff.index[:5]:
            top_features.append(
                {
                    "feature": fname,
                    "cluster_mean": float(cluster_means[fname]),
                    "overall_mean": float(overall_means[fname]),
                }
            )

        clusters_out.append(
            {
                "cluster_id": cid,
                "size": n_cluster,
                "ratio": frac,
                "performance_mean": perf_mean,
                "performance_overall": perf_overall,
                "top_diff_features": top_features,
            }
        )

    return {
        "k": k,
        "n_rows": n_total,
        "numeric_features": num_cols,
        "clusters": clusters_out,
    }




@app.get("/api/model_summary")
async def api_model_summary():
    """
    å›å‚³ç›®å‰å·²è¨“ç·´æ¨¡å‹çš„æ‘˜è¦è³‡è¨Šï¼š
    - metricsï¼šRF / ElasticNet çš„ R2 / MAE
    - top_featuresï¼šå‰å¹¾å€‹ SHAP é‡è¦ç‰¹å¾µï¼ˆå«é è¨­å€¼èˆ‡æç¤ºï¼‰
    """
    if MODEL_STATE["pipe_rf"] is None:
        raise HTTPException(
            status_code=400,
            detail="å°šæœªè¨“ç·´æ¨¡å‹ï¼Œè«‹å…ˆåˆ°ã€Œé æ¸¬ Demo é ã€ä¸Šå‚³è³‡æ–™ä¸¦å»ºç«‹æ¨¡å‹ã€‚",
        )

    return {
        "has_model": True,
        "has_shap": HAS_SHAP,
        "metrics": MODEL_STATE.get("metrics") or {},
        "top_features": MODEL_STATE.get("top_features") or [],
        "feature_cols": MODEL_STATE.get("feature_cols") or [],
    }

# ===== éœæ…‹é é¢ï¼ˆRender éƒ¨ç½²å‹å–„ï¼šåŒä¸€å€‹æœå‹™åŒæ™‚æä¾› API + HTMLï¼‰=====
_BASE_DIR = Path(__file__).resolve().parent
_STATIC_DIR = _BASE_DIR / "static"

if _STATIC_DIR.exists():
    # html=True æœƒè®“ / è‡ªå‹•å›å‚³ index.html
    app.mount("/", StaticFiles(directory=str(_STATIC_DIR), html=True), name="static")

